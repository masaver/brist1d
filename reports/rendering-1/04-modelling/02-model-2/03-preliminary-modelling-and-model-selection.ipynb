{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preliminary modelling and model selection",
   "id": "229aae3a0b54a1be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T11:31:03.808307Z",
     "start_time": "2024-12-03T11:30:16.688509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from src.features.helpers.load_data import load_data\n",
    "from src.models.model_2.model.pipelines_2h import pipeline\n",
    "\n",
    "train_data, augmented_data, test_data = load_data('2_00h')\n",
    "\n",
    "all_train_data_transformed = pipeline.fit_transform(pd.concat([train_data, augmented_data]))\n",
    "\n",
    "X_train, y_train = all_train_data_transformed.iloc[len(train_data):].drop(columns=['bg+1:00']), all_train_data_transformed.iloc[len(train_data):]['bg+1:00']\n",
    "X_augmented, y_augmented = all_train_data_transformed.iloc[:len(train_data)].drop(columns=['bg+1:00']), all_train_data_transformed.iloc[:len(train_data)]['bg+1:00']"
   ],
   "id": "69813d7ee3b36a8a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Lazy Predict",
   "id": "64a49dc6577a1818"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To get some fast insights about the results of different regression models we can use the `LazyPredict` library. This library trains a set of regression models on the data and return the results. This can be used to get a quick overview of the performance of different models. For our usecase we wrote wrapper araound LazyPredict to have more control over the models applied. ",
   "id": "245cd48b6334e455"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T09:19:02.801672Z",
     "start_time": "2024-12-03T09:19:02.437102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.features.helpers.LazyPredict import get_lazy_regressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_augmented_train, X_augmented_test, y_augmented_train, y_augmented_test = train_test_split(X_augmented, y_augmented, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_all = pd.concat([X_train, X_augmented_train], axis=0)\n",
    "y_train_all = pd.concat([y_train, y_augmented_train], axis=0)\n",
    "\n",
    "lazy_predict_regressor = get_lazy_regressor(exclude=['SVN'])\n",
    "models, predictions = lazy_predict_regressor.fit(X_train=X_train_all, y_train=y_train_all, X_test=X_augmented_test, y_test=y_augmented_test)\n",
    "models"
   ],
   "id": "ac0f0887009b1ac9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Selection\n",
    "\n",
    "Based on the results from the `LazyPredict` library we can select the best performing models from each category, fine tune them and use them in a `StackingRegressor` for the final prediction. \n",
    "\n",
    "In this case we will use:\n",
    " \n",
    "* `HistGradientBoostingRegressor`\n",
    "* `LassoLarsICRegressor`\n",
    "* `KNNRegressor`\n",
    "* `XGBRegressor`\n",
    "\n",
    "### Why HistGradientBoostingRegressor\n",
    "\n",
    "Category: Gradient Boosting Model\n",
    "\n",
    "Strengths:\n",
    "* Efficient implementation of gradient boosting, optimized for large datasets with categorical features.\n",
    "* Handles missing data well and works effectively on high-dimensional datasets.\n",
    "* Generally robust to overfitting due to regularization.\n",
    " \n",
    "Unique Contribution to Stacking:\n",
    "* Captures complex, non-linear relationships.\n",
    "* Performs well in terms of accuracy on structured data and integrates nicely with other weaker models.\n",
    "\n"
   ],
   "id": "706f75125d78cbc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Why LassoLarsICRegressor\n",
    "\n",
    "Category: Linear Model\n",
    "\n",
    "Strengths: \n",
    "* A regression model that combines Lasso (L1 regularization) with a model selection method based on the Akaike Information Criterion (AIC) or Bayes Information Criterion (BIC). \n",
    "* Particularly effective for datasets with a large number of features but where most coefficients are zero (sparse datasets).\n",
    "\n",
    "Unique Contribution to Stacking:\n",
    "* Provides a strong linear baseline that helps the ensemble learn from both linear trends and more complex patterns captured by non-linear models.\n",
    "* Avoids overfitting by feature selection.\n",
    "\n"
   ],
   "id": "7daf7ac1c46d8d0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Why KNNRegressor\n",
    "\n",
    "Category: Instance-Based Learning (Non-parametric)\n",
    "\n",
    "Strengths: \n",
    "* Simple yet effective for small-to-medium datasets, particularly when the relationship between features and the target variable is highly localized.\n",
    "* Handles non-linear relationships without assuming any prior distribution.\n",
    "\n",
    "Unique Contribution to Stacking:\n",
    "* Introduces local prediction capability that complements global models like gradient boosting.\n",
    "* Offers diversity to the ensemble, as its predictions are based purely on similarity rather than a parametric model. "
   ],
   "id": "3ddc321f7aff2b17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Why XGBRegressor\n",
    "\n",
    "Category: Gradient Boosting Model\n",
    "\n",
    "Strengths:\n",
    "* High performance and efficiency due to optimized implementations.\n",
    "* Supports a variety of tuning options and regularization techniques (L1 and L2).\n",
    "* Often achieves state-of-the-art results in many regression tasks.\n",
    "\n",
    "Unique Contribution to Stacking:\n",
    "* Brings additional predictive power, especially when the dataset has complex feature interactions.\n",
    "* Complements HistGradientBoostingRegressor by leveraging different gradient boosting frameworks.\n",
    "\n",
    "\n",
    "### Why Stacking these models\n",
    "\n",
    "1. Diversity: Each model belongs to a different category (linear, boosting, instance-based), ensuring diverse perspectives.\n",
    "2. Complementary Strengths: Combining models that excel in different aspects (e.g., linear vs. non-linear, global vs. local) leads to a well-rounded regressor.\n",
    "3. Error Reduction: Aggregating predictions mitigates individual model weaknesses, reducing bias and variance.\n",
    "4. Improved Generalization: Stacking effectively captures patterns that individual models may miss, enhancing overall performance.\n",
    "\n",
    "\n",
    "By carefully tuning these models and stacking their predictions, the ensemble benefits from their combined strengths, yielding a robust and accurate final regressor.\n",
    "\n"
   ],
   "id": "194abd2d93bb40d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ca9dd7e6807e75fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
