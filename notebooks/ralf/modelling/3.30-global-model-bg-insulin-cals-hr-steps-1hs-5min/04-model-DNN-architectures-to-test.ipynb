{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DNN estimator architectures and their performance evaluation\n",
   "id": "c4f5fa4ffbc53369"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "adca5bda0175c185"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T22:07:50.494497Z",
     "start_time": "2024-11-28T22:06:30.410487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from data import load_data_selected_features\n",
    "from pipelines_selected_features import pipeline\n",
    "\n",
    "train_data, additional_train_data, test_data = load_data_selected_features()\n",
    "all_train_data = pipeline.fit_transform(pd.concat([train_data, additional_train_data]))\n",
    "\n",
    "# cut the data into train, additional train and test\n",
    "train_data = all_train_data.loc[train_data.index]\n",
    "additional_train_data = all_train_data.loc[additional_train_data.index]\n",
    "\n",
    "X_train = train_data.drop(columns=['bg+1:00'])\n",
    "y_train = train_data['bg+1:00']\n",
    "\n",
    "X_additional_train = additional_train_data.drop(columns=['bg+1:00'])\n",
    "y_additional_train = additional_train_data['bg+1:00']"
   ],
   "id": "6a34b21d4bb55fe0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MA-01 (used in v4.0)",
   "id": "ba125fba40cfb8cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T22:36:08.209865Z",
     "start_time": "2024-11-28T22:07:50.572461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from model_performance_calculations import calculate_dnn_performance, get_rmse_boxplot_chart, get_rmse_line_chart, get_history_line_chart\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def create_dnn_model(input_dimension: int):\n",
    "    dnn = Sequential([\n",
    "        Input(shape=(input_dimension,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    dnn.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[rmse]\n",
    "    )\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "score, histories = calculate_dnn_performance(create_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=3, epochs=60, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "454a23fa1b7c57e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1054/1054\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 798us/step\n",
      "\u001B[1m1054/1054\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 361us/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 39\u001B[0m\n\u001B[1;32m     30\u001B[0m     dnn\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[1;32m     31\u001B[0m         optimizer\u001B[38;5;241m=\u001B[39mAdam(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m),\n\u001B[1;32m     32\u001B[0m         loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmse\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     33\u001B[0m         metrics\u001B[38;5;241m=\u001B[39m[rmse]\n\u001B[1;32m     34\u001B[0m     )\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dnn\n\u001B[0;32m---> 39\u001B[0m score, histories \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate_dnn_performance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcreate_dnn_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_additional_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_additional_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m60\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m get_history_line_chart(histories)\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[0;32m~/Projects/learning-projects/datascience-bootcamp/sep24_bds_int_medical/notebooks/ralf/modelling/3.30-global-model-bg-insulin-cals-hr-steps-1hs-5min/model_performance_calculations.py:162\u001B[0m, in \u001B[0;36mcalculate_dnn_performance\u001B[0;34m(create_model_fn, X_train, y_train, X_additional_train, y_additional_train, verbose, n_splits, epochs, groups, callbacks)\u001B[0m\n\u001B[1;32m    158\u001B[0m y_train_split, y_test_split \u001B[38;5;241m=\u001B[39m y_additional_train\u001B[38;5;241m.\u001B[39miloc[train_index], y_additional_train\u001B[38;5;241m.\u001B[39miloc[test_index]\n\u001B[1;32m    160\u001B[0m model \u001B[38;5;241m=\u001B[39m create_model_fn(X_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m--> 162\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train_split\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_split\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_test_split\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test_split\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\n\u001B[1;32m    169\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    171\u001B[0m histories\u001B[38;5;241m.\u001B[39mappend(model\u001B[38;5;241m.\u001B[39mhistory\u001B[38;5;241m.\u001B[39mhistory)\n\u001B[1;32m    172\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test_split)\n",
      "File \u001B[0;32m~/Projects/learning-projects/datascience-bootcamp/sep24_bds_int_medical/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Projects/learning-projects/datascience-bootcamp/sep24_bds_int_medical/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001B[0m, in \u001B[0;36mTensorFlowTrainer.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, iterator \u001B[38;5;129;01min\u001B[39;00m epoch_iterator\u001B[38;5;241m.\u001B[39menumerate_epoch():\n\u001B[1;32m    319\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m--> 320\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_end(step, logs)\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n",
      "File \u001B[0;32m~/Projects/learning-projects/datascience-bootcamp/sep24_bds_int_medical/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Projects/learning-projects/datascience-bootcamp/sep24_bds_int_medical/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    830\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 833\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    835\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    836\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/Projects/learning-projects/datascience-bootcamp/sep24_bds_int_medical/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    875\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    876\u001B[0m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[1;32m    877\u001B[0m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[0;32m--> 878\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_variable_creation_config\u001B[49m\n\u001B[1;32m    880\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    881\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables:\n\u001B[1;32m    882\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    883\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Projects/learning-projects/datascience-bootcamp/sep24_bds_int_medical/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:137\u001B[0m, in \u001B[0;36mcall_function\u001B[0;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[1;32m    132\u001B[0m function \u001B[38;5;241m=\u001B[39m trace_function(\n\u001B[1;32m    133\u001B[0m     args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs, tracing_options\u001B[38;5;241m=\u001B[39mtracing_options\n\u001B[1;32m    134\u001B[0m )\n\u001B[1;32m    136\u001B[0m \u001B[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001B[39;00m\n\u001B[0;32m--> 137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m \u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m function\u001B[38;5;241m.\u001B[39m_call_flat(  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    140\u001B[0m     flat_inputs, captured_inputs\u001B[38;5;241m=\u001B[39mfunction\u001B[38;5;241m.\u001B[39mcaptured_inputs\n\u001B[1;32m    141\u001B[0m )\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/inspect.py:3268\u001B[0m, in \u001B[0;36mSignature.bind\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3262\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m   3263\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgot an unexpected keyword argument \u001B[39m\u001B[38;5;132;01m{arg!r}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   3264\u001B[0m                     arg\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(kwargs))))\n\u001B[1;32m   3266\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_arguments_cls(\u001B[38;5;28mself\u001B[39m, arguments)\n\u001B[0;32m-> 3268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbind\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m/\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   3269\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001B[39;00m\n\u001B[1;32m   3270\u001B[0m \u001B[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001B[39;00m\n\u001B[1;32m   3271\u001B[0m \u001B[38;5;124;03m    if the passed arguments can not be bound.\u001B[39;00m\n\u001B[1;32m   3272\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   3273\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bind(args, kwargs)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MA-01 Improvements (used in 5.1)\n",
    "\n",
    "Reduce complexity of the model by removing the lowest layer\n",
    "Try to get a stable RMSE arond 2"
   ],
   "id": "349ab03989673f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add Regularization, remove lowest layer",
   "id": "c1cb6b13ef40dd2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from model_performance_calculations import calculate_dnn_performance, get_rmse_boxplot_chart, get_rmse_line_chart, get_history_line_chart\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def create_dnn_model(input_dimension: int):\n",
    "    dnn = Sequential([\n",
    "        Input(shape=(input_dimension,)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    dnn.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[rmse]\n",
    "    )\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(create_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=3, epochs=100, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "b87907d15138fb85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MA-02\n",
    "### Increased Neurons in Dense Layers"
   ],
   "id": "6324582915348f0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from model_performance_calculations import calculate_dnn_performance, get_rmse_boxplot_chart, get_rmse_line_chart, get_history_line_chart\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def create_dnn_model(input_dimension: int):\n",
    "    dnn = Sequential([\n",
    "        # Input layer\n",
    "        Input(shape=(input_dimension,)),\n",
    "\n",
    "        # First dense block\n",
    "        Dense(512, activation='relu'),  # Increased neurons\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),  # Slightly reduced dropout for better capacity\n",
    "\n",
    "        # Second dense block\n",
    "        Dense(256, activation='relu'),  # Increased neurons\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # Third dense block\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # Fourth dense block (new)\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # Output layer\n",
    "        Dense(1, activation='linear')  # Regression output\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    dnn.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),  # Retained learning rate\n",
    "        loss='mse',  # Mean Squared Error for regression\n",
    "        metrics=[rmse]  # Custom RMSE metric\n",
    "    )\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(create_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5, epochs=100, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "cea73fb247528f6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Adjusted Layer Sizes with More Nonlinearity",
   "id": "f76a33c3e0fdaa18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from model_performance_calculations import calculate_dnn_performance, get_rmse_boxplot_chart, get_rmse_line_chart, get_history_line_chart\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def create_dnn_model(input_dimension: int):\n",
    "    dnn = Sequential([\n",
    "        Input(shape=(input_dimension,)),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    dnn.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[rmse]\n",
    "    )\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(create_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5, epochs=100, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "76ef16d3451885e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Switched to Exponential Linear Unit (ELU) activation function",
   "id": "74b2d422a37f3f68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from model_performance_calculations import calculate_dnn_performance, get_rmse_boxplot_chart, get_rmse_line_chart, get_history_line_chart\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def create_dnn_model(input_dimension: int):\n",
    "    dnn = Sequential([\n",
    "        Input(shape=(input_dimension,)),\n",
    "        Dense(128, activation='elu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64, activation='elu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(32, activation='elu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    dnn.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[rmse]\n",
    "    )\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(create_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5, epochs=100, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "835a7dacbb08dff2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Residual Connections (Inspired by ResNet)",
   "id": "1ecdcf82173a083f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Add, Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from model_performance_calculations import calculate_dnn_performance, get_rmse_boxplot_chart, get_rmse_line_chart, get_history_line_chart\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Add\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def create_dnn_model(input_dimension: int):\n",
    "    def residual_block(input_layer, units):\n",
    "        # First dense layer\n",
    "        dense1 = Dense(units, activation='relu')(input_layer)\n",
    "        batchnorm1 = BatchNormalization()(dense1)\n",
    "        # Second dense layer\n",
    "        dense2 = Dense(units, activation='relu')(batchnorm1)\n",
    "        batchnorm2 = BatchNormalization()(dense2)\n",
    "        # Add projection if shapes don't match\n",
    "        if input_layer.shape[-1] != units:\n",
    "            input_layer = Dense(units)(input_layer)  # Project input to match dimensions\n",
    "        # Add the skip connection\n",
    "        return Add()([input_layer, batchnorm2])\n",
    "\n",
    "    input_layer = Input(shape=(input_dimension,))\n",
    "    residual1 = residual_block(input_layer, 128)\n",
    "    residual2 = residual_block(residual1, 64)\n",
    "    residual3 = residual_block(residual2, 32)\n",
    "\n",
    "    output = Dense(1, activation='linear')(residual3)\n",
    "\n",
    "    dnn = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    dnn.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[rmse]\n",
    "    )\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(create_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5, epochs=100, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "631afd56ad52786e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Advanced Optimizer: Learning Rate Scheduler",
   "id": "cba3d8bcdeefb76b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from model_performance_calculations import calculate_dnn_performance, get_rmse_boxplot_chart, get_rmse_line_chart, get_history_line_chart\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.9\n",
    "\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "def create_dnn_model(input_dimension: int):\n",
    "    dnn = Sequential([\n",
    "        Input(shape=(input_dimension,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    dnn.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[rmse]\n",
    "    )\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(create_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5, epochs=100, callbacks=[lr_scheduler], verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "39e348f7146ed199",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hybrid Model (Combining Linear and Nonlinear Components)\n",
    "\n"
   ],
   "id": "484ee947ed1b89f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from model_performance_calculations import calculate_dnn_performance, get_rmse_boxplot_chart, get_rmse_line_chart, get_history_line_chart\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def create_dnn_model(input_dimension: int):\n",
    "    input_layer = Input(shape=(input_dimension,))\n",
    "\n",
    "    # Linear component\n",
    "    linear = Dense(1, activation='linear')(input_layer)\n",
    "\n",
    "    # Nonlinear component\n",
    "    nonlinear = Dense(128, activation='relu')(input_layer)\n",
    "    nonlinear = BatchNormalization()(nonlinear)\n",
    "    nonlinear = Dropout(0.3)(nonlinear)\n",
    "    nonlinear = Dense(64, activation='relu')(nonlinear)\n",
    "    nonlinear = BatchNormalization()(nonlinear)\n",
    "    nonlinear = Dropout(0.3)(nonlinear)\n",
    "    nonlinear = Dense(32, activation='relu')(nonlinear)\n",
    "    nonlinear = BatchNormalization()(nonlinear)\n",
    "\n",
    "    # Merge linear and nonlinear components\n",
    "    merged = Concatenate()([linear, nonlinear])\n",
    "    output = Dense(1, activation='linear')(merged)\n",
    "\n",
    "    dnn = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    dnn.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[rmse]\n",
    "    )\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(create_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5, epochs=100, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "cd3ab4b452eb76e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Others Optimizers",
   "id": "5cf9b4d4a05f139a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from model_performance_calculations import calculate_dnn_performance, get_rmse_boxplot_chart, get_rmse_line_chart, get_history_line_chart\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def create_dnn_model(input_dimension: int):\n",
    "    dnn = Sequential([\n",
    "        # Input Layer\n",
    "        Input(shape=(input_dimension,)),\n",
    "        Dense(128),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        # Hidden Layer 1\n",
    "        Dense(64),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        # Hidden Layer 2    \n",
    "        Dense(32),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        # Output Layer\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    dnn.compile(optimizer='adam', loss='mse', metrics=[rmse])\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(create_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5, epochs=100, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "481ce618f53ba207",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate a DNN Model with Early Stopping",
   "id": "10d27a7b29617a0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from model_performance_calculations import calculate_dnn_performance, get_rmse_boxplot_chart, get_rmse_line_chart, get_history_line_chart\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def create_dnn_model(input_dimension: int):\n",
    "    dnn = Sequential([\n",
    "        Input(shape=(input_dimension,)),\n",
    "        Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    dnn.compile(optimizer='adam', loss='mse', metrics=[rmse])\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(create_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5, epochs=100, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "b0c106bcfdca4972",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simple and wide model",
   "id": "7062a21f117a4be6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from model_performance_calculations import calculate_dnn_performance, get_rmse_boxplot_chart, get_rmse_line_chart, get_history_line_chart\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def create_dnn_model(input_dimension: int):\n",
    "    dnn = Sequential([\n",
    "        Input(shape=(input_dimension,)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    dnn.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[rmse]\n",
    "    )\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(create_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5, epochs=100, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "d0b2e7e24220e2ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Shallow with Feature Compression",
   "id": "a383b683794c5a99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from model_performance_calculations import calculate_dnn_performance, get_rmse_boxplot_chart, get_rmse_line_chart, get_history_line_chart\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def create_dnn_model(input_dimension: int):\n",
    "    dnn = Sequential([\n",
    "        Input(shape=(input_dimension,)),\n",
    "        Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    dnn.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[rmse]\n",
    "    )\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(create_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5, epochs=100, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "b5ef636b431d01df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wider Network (No Gradual Reduction)",
   "id": "a16242d1bfc9a3c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def get_dnn_model(input_dimension: int):\n",
    "    dnn = Sequential([\n",
    "        Input(shape=(input_dimension,)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    dnn.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[rmse]\n",
    "    )\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(get_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5, epochs=100, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "18fbbd1cd6632410",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wide and Deep Model",
   "id": "f1001a4c774a18ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "\n",
    "def get_dnn_model(input_dimension: int):\n",
    "    input_layer = Input(shape=(input_dimension,))\n",
    "    wide = Dense(128, activation='relu')(input_layer)  # Wide component\n",
    "    deep = Dense(128, activation='relu')(input_layer)\n",
    "    deep = Dense(64, activation='relu')(deep)\n",
    "    deep = Dense(32, activation='relu')(deep)\n",
    "\n",
    "    merged = Concatenate()([wide, deep])\n",
    "    output_layer = Dense(1, activation='linear')(merged)\n",
    "\n",
    "    dnn = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    dnn.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[rmse]\n",
    "    )\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(get_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5, epochs=100, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "cfd8ebbcf29a4d6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Regularized Wide & Deep Model",
   "id": "e30d03990f7b5984"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from model_performance_calculations import calculate_dnn_performance, get_rmse_boxplot_chart, get_rmse_line_chart, get_history_line_chart\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def get_dnn_model(input_dimension: int):\n",
    "    input_layer = Input(shape=(input_dimension,))\n",
    "\n",
    "    # Wide part of the model with L2 regularization and Dropout\n",
    "    wide = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "    wide = Dropout(0.5)(wide)\n",
    "    wide = BatchNormalization()(wide)\n",
    "\n",
    "    # Deep part of the model with L2 regularization, Dropout, and BatchNormalization\n",
    "    deep = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "    deep = Dropout(0.5)(deep)\n",
    "    deep = BatchNormalization()(deep)\n",
    "\n",
    "    deep = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(deep)\n",
    "    deep = Dropout(0.5)(deep)\n",
    "    deep = BatchNormalization()(deep)\n",
    "\n",
    "    deep = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(deep)\n",
    "    deep = Dropout(0.5)(deep)\n",
    "    deep = BatchNormalization()(deep)\n",
    "\n",
    "    # Merge the wide and deep components\n",
    "    merged = Concatenate()([wide, deep])\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(1, activation='linear')(merged)\n",
    "\n",
    "    # Create the model\n",
    "    dnn_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile the model\n",
    "    dnn_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[rmse]\n",
    "    )\n",
    "\n",
    "    return dnn_model\n",
    "\n",
    "\n",
    "_, histories = calculate_dnn_performance(get_dnn_model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5, epochs=100, verbose=0)\n",
    "get_history_line_chart(histories).show()"
   ],
   "id": "dae608d3836b44d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
