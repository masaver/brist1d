{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tune the meta model\n",
    " \n",
    "We want to check if we can exchange the metamodel from Ridge to DNN\n",
    "\n",
    "We use the best base model parameters from the previous notebooks"
   ],
   "id": "32bfa9234ec39c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model v4.0 - Baseline model with added DNN base estimator",
   "id": "c4f5fa4ffbc53369"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:48:03.788783Z",
     "start_time": "2024-11-28T09:47:48.790264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from fileinput import filename\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from data import load_data_selected_features\n",
    "\n",
    "train_data, additional_train_data, test_data = load_data_selected_features()"
   ],
   "id": "ed9cab10a4c9269",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:48:03.809994Z",
     "start_time": "2024-11-28T09:48:03.797488Z"
    }
   },
   "cell_type": "code",
   "source": "train_data.info()",
   "id": "af6bdd55ecd2a72b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 150996 entries, p01_0 to p12_25298\n",
      "Columns: 507 entries, p_num to bg+1:00\n",
      "dtypes: float64(433), object(74)\n",
      "memory usage: 585.2+ MB\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:48:03.968469Z",
     "start_time": "2024-11-28T09:48:03.957441Z"
    }
   },
   "cell_type": "code",
   "source": "additional_train_data.info()",
   "id": "e6efef1597276890",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 168588 entries, p01_test_12 to p24_test_94938\n",
      "Columns: 507 entries, p_num to bg+1:00\n",
      "dtypes: float64(433), object(74)\n",
      "memory usage: 657.4+ MB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:48:03.992268Z",
     "start_time": "2024-11-28T09:48:03.981762Z"
    }
   },
   "cell_type": "code",
   "source": "test_data.info()",
   "id": "16d75f6d697d8bc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3644 entries, p01_8459 to p24_260\n",
      "Columns: 506 entries, p_num to activity-0:00\n",
      "dtypes: float64(432), object(74)\n",
      "memory usage: 14.2+ MB\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare the data",
   "id": "fc5e077e5738d8eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:48:47.967987Z",
     "start_time": "2024-11-28T09:48:04.008642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pipelines_selected_features import pipeline\n",
    "\n",
    "all_train_data = pd.concat([train_data, additional_train_data], axis=0)\n",
    "all_train_data_transformed = pipeline.fit_transform(all_train_data)\n",
    "\n",
    "train_data_transformed = all_train_data_transformed[:len(train_data)]\n",
    "additional_train_data_transformed = all_train_data_transformed[len(train_data):]\n",
    "\n",
    "X_train = train_data_transformed.drop(columns=['bg+1:00'])\n",
    "y_train = train_data_transformed['bg+1:00']\n",
    "\n",
    "X_additional_train = additional_train_data_transformed.drop(columns=['bg+1:00'])\n",
    "y_additional_train = additional_train_data_transformed['bg+1:00']"
   ],
   "id": "c7df704ede80b62",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predict for validation data and check results",
   "id": "9b608adb90b0235e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "662b261cf2ae072"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:48:50.955724Z",
     "start_time": "2024-11-28T09:48:47.996688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def create_dnn_model(input_dimension: int):\n",
    "    dnn = Sequential([\n",
    "        # Input layer\n",
    "        Input(shape=(input_dimension,)),\n",
    "\n",
    "        # First dense block\n",
    "        Dense(512, activation='relu'),  # Increased neurons\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),  # Slightly reduced dropout for better capacity\n",
    "\n",
    "        # Second dense block\n",
    "        Dense(256, activation='relu'),  # Increased neurons\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # Third dense block\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # Fourth dense block (new)\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # Output layer\n",
    "        Dense(1, activation='linear')  # Regression output\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    dnn.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),  # Retained learning rate\n",
    "        loss='mse',  # Mean Squared Error for regression\n",
    "        metrics=[rmse]  # Custom RMSE metric\n",
    "    )\n",
    "\n",
    "    return dnn\n",
    "\n",
    "\n",
    "def model_with_pretrained_weights():\n",
    "    dnn = create_dnn_model(X_train.shape[1])\n",
    "    dnn.load_weights('v4.0-dnn.weights.h5')  # Load the pre-trained weights\n",
    "\n",
    "    # Freeze all layers except the last one (optional)\n",
    "    for layer in dnn.layers:\n",
    "        layer.trainable = False\n",
    "    # Unfreeze the output layer if you want to fine-tune it\n",
    "    # dnn.layers[-1].trainable = True\n",
    "\n",
    "    dnn.compile(optimizer='adam', loss='mse', metrics=[rmse])\n",
    "    return dnn\n"
   ],
   "id": "c20aa25cccb09977",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:48:51.397904Z",
     "start_time": "2024-11-28T09:48:50.982581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import StackingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import LassoLarsIC, Ridge\n",
    "\n",
    "hgb_estimator = HistGradientBoostingRegressor(max_iter=200, max_depth=5, learning_rate=0.1)\n",
    "lasso_lars_ic_base_model = LassoLarsIC(criterion='bic', max_iter=10000)\n",
    "knn_base_model = KNeighborsRegressor(n_neighbors=5)\n",
    "xgb_base_model = XGBRegressor(objective='reg:squarederror', random_state=42, n_estimators=500, max_depth=5, learning_rate=0.1)\n",
    "\n",
    "# pretrain the DNN and use the weights in the stacking model\n",
    "pretrained_dnn = create_dnn_model(X_train.shape[1])\n",
    "pretrained_dnn.fit(\n",
    "    pd.concat([X_train, X_additional_train], axis=0),\n",
    "    pd.concat([y_train, y_additional_train], axis=0),\n",
    "    epochs=100, verbose=2\n",
    ")\n",
    "pretrained_dnn.save_weights('v4.0-dnn.weights.h5')\n",
    "\n",
    "keras_regressor = KerasRegressor(\n",
    "    model=model_with_pretrained_weights,\n",
    "    epochs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "estimators = [\n",
    "    ('hgb', hgb_estimator),\n",
    "    ('lasso_lars_ic', lasso_lars_ic_base_model),\n",
    "    ('knn', knn_base_model),\n",
    "    ('xgb', xgb_base_model),\n",
    "    ('dnn', keras_regressor)\n",
    "]\n",
    "\n",
    "model = StackingRegressor(estimators=estimators, final_estimator=Ridge(alpha=0.1), n_jobs=1, verbose=2)"
   ],
   "id": "7e0583e9ae957c2d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m xgb_base_model \u001B[38;5;241m=\u001B[39m XGBRegressor(objective\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreg:squarederror\u001B[39m\u001B[38;5;124m'\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m, max_depth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# pretrain the DNN and use the weights in the stacking model\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m pretrained_dnn \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_dnn_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m pretrained_dnn\u001B[38;5;241m.\u001B[39mfit(pd\u001B[38;5;241m.\u001B[39mconcat([X_train, X_additional_train], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m), pd\u001B[38;5;241m.\u001B[39mconcat([y_train, y_additional_train], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m), epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     14\u001B[0m pretrained_dnn\u001B[38;5;241m.\u001B[39msave_weights(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mv4.0-dnn.weights.h5\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[6], line 14\u001B[0m, in \u001B[0;36mcreate_dnn_model\u001B[0;34m(input_dimension)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_dnn_model\u001B[39m(input_dimension: \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m---> 14\u001B[0m     dnn \u001B[38;5;241m=\u001B[39m \u001B[43mSequential\u001B[49m([\n\u001B[1;32m     15\u001B[0m         \u001B[38;5;66;03m# Input layer\u001B[39;00m\n\u001B[1;32m     16\u001B[0m         Input(shape\u001B[38;5;241m=\u001B[39m(input_dimension,)),\n\u001B[1;32m     17\u001B[0m \n\u001B[1;32m     18\u001B[0m         \u001B[38;5;66;03m# First dense block\u001B[39;00m\n\u001B[1;32m     19\u001B[0m         Dense(\u001B[38;5;241m512\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m),  \u001B[38;5;66;03m# Increased neurons\u001B[39;00m\n\u001B[1;32m     20\u001B[0m         BatchNormalization(),\n\u001B[1;32m     21\u001B[0m         Dropout(\u001B[38;5;241m0.3\u001B[39m),  \u001B[38;5;66;03m# Slightly reduced dropout for better capacity\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \n\u001B[1;32m     23\u001B[0m         \u001B[38;5;66;03m# Second dense block\u001B[39;00m\n\u001B[1;32m     24\u001B[0m         Dense(\u001B[38;5;241m256\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m),  \u001B[38;5;66;03m# Increased neurons\u001B[39;00m\n\u001B[1;32m     25\u001B[0m         BatchNormalization(),\n\u001B[1;32m     26\u001B[0m         Dropout(\u001B[38;5;241m0.3\u001B[39m),\n\u001B[1;32m     27\u001B[0m \n\u001B[1;32m     28\u001B[0m         \u001B[38;5;66;03m# Third dense block\u001B[39;00m\n\u001B[1;32m     29\u001B[0m         Dense(\u001B[38;5;241m128\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m     30\u001B[0m         BatchNormalization(),\n\u001B[1;32m     31\u001B[0m         Dropout(\u001B[38;5;241m0.3\u001B[39m),\n\u001B[1;32m     32\u001B[0m \n\u001B[1;32m     33\u001B[0m         \u001B[38;5;66;03m# Fourth dense block (new)\u001B[39;00m\n\u001B[1;32m     34\u001B[0m         Dense(\u001B[38;5;241m64\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m     35\u001B[0m         BatchNormalization(),\n\u001B[1;32m     36\u001B[0m         Dropout(\u001B[38;5;241m0.3\u001B[39m),\n\u001B[1;32m     37\u001B[0m \n\u001B[1;32m     38\u001B[0m         \u001B[38;5;66;03m# Output layer\u001B[39;00m\n\u001B[1;32m     39\u001B[0m         Dense(\u001B[38;5;241m1\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinear\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# Regression output\u001B[39;00m\n\u001B[1;32m     40\u001B[0m     ])\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;66;03m# Compile the model\u001B[39;00m\n\u001B[1;32m     43\u001B[0m     dnn\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[1;32m     44\u001B[0m         optimizer\u001B[38;5;241m=\u001B[39mAdam(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m),  \u001B[38;5;66;03m# Retained learning rate\u001B[39;00m\n\u001B[1;32m     45\u001B[0m         loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmse\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# Mean Squared Error for regression\u001B[39;00m\n\u001B[1;32m     46\u001B[0m         metrics\u001B[38;5;241m=\u001B[39m[rmse]  \u001B[38;5;66;03m# Custom RMSE metric\u001B[39;00m\n\u001B[1;32m     47\u001B[0m     )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from model_performance_calculations import calculate_stacking_regressor_performance, get_rmse_boxplot_chart, get_rmse_line_chart, save_performances, save_model\n",
    "\n",
    "date_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_name = f'{date_time}-model-v4.0-DNN'\n",
    "\n",
    "save_model(model, os.path.join('models', f'{model_name}.pkl'))\n",
    "\n",
    "performances = calculate_stacking_regressor_performance(model, X_train, y_train, X_additional_train, y_additional_train, n_splits=5)\n",
    "save_performances(performances, os.path.join('models', f'{model_name}-performances.json'))\n",
    "\n",
    "get_rmse_boxplot_chart(performances).show()\n",
    "get_rmse_line_chart(performances).show()"
   ],
   "id": "c0be6db84ab0c1b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from pipelines_selected_features import pipeline\n",
    "\n",
    "all_train_data_transformed = pipeline.fit_transform(pd.concat([train_data, additional_train_data], axis=0))\n",
    "\n",
    "X_train = all_train_data_transformed.drop(columns=['bg+1:00'])\n",
    "y_train = all_train_data_transformed['bg+1:00']\n",
    "X_test = pipeline.transform(test_data)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ],
   "id": "43cf99724f468af4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_pred = model.predict(X_test)\n",
    "test_data['bg+1:00'] = y_pred\n",
    "\n",
    "if np.sum(y_pred < 0) > 0:\n",
    "    print(f'Number of negative values: {np.sum(y_pred < 0)}')\n",
    "    bg_min_train = np.min(y_train)\n",
    "    print(f'Min value: {np.min(y_pred)}')\n",
    "    print(f'Filling negative values with {bg_min_train}')\n",
    "    y_pred = np.where(y_pred < 0, bg_min_train, y_pred)\n",
    "\n",
    "test_data['bg+1:00'] = y_pred\n",
    "test_data.head()"
   ],
   "id": "4bc5c6a7c86b6f3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "submission = pd.DataFrame(test_data['bg+1:00'])\n",
    "submission"
   ],
   "id": "5167875e1afa3ec1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "submission.to_csv(f'submission-{os.path.basename(os.getcwd())}.csv')",
   "id": "7b61741c1c7498d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    ",
   "id": "ff56b64b83993e6c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
